{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5428b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import joblib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "\n",
    "# ============================\n",
    "# Configuration & Setup\n",
    "# ============================\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "data_path = \"/Users/darshan__6122__/data_cleaning_project/Parth/AmazonReviews/train.csv\"\n",
    "log_file = \"model_comparison_log.csv\"\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    raise FileNotFoundError(f\"Dataset not found at {data_path}.\")\n",
    "\n",
    "print(\"Dataset located. Loading...\")\n",
    "\n",
    "# ============================\n",
    "# Load and Preprocess Dataset\n",
    "# ============================\n",
    "\n",
    "data = pd.read_csv(\n",
    "    data_path,\n",
    "    names=[\"polarity\", \"title\", \"text\"],\n",
    "    dtype={\"polarity\": \"int8\", \"title\": \"string\", \"text\": \"string\"},\n",
    "    usecols=[\"polarity\", \"text\"],\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "data[\"polarity\"] = data[\"polarity\"].map({1: 0, 2: 1}).astype(\"int8\")\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9',.!?]\", \" \", text)\n",
    "    return text.lower()\n",
    "\n",
    "data[\"text\"] = data[\"text\"].astype(str).map(clean_text)\n",
    "\n",
    "# ============================\n",
    "# Feature Extraction\n",
    "# ============================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"text\"], data[\"polarity\"], test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "vocab_size = 5000\n",
    "max_length = 80\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_padded = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_length, padding=\"post\")\n",
    "X_test_padded = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_length, padding=\"post\")\n",
    "\n",
    "# ============================\n",
    "# Model Training & Loading\n",
    "# ============================\n",
    "\n",
    "models = {}\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model_path = \"logistic_regression.pkl\"\n",
    "if os.path.exists(lr_model_path):\n",
    "    print(\"Loading Logistic Regression model...\")\n",
    "    lr_model = joblib.load(lr_model_path)\n",
    "    lr_model.fit(X_train_tfidf, y_train)\n",
    "else:\n",
    "    print(\"Training Logistic Regression model...\")\n",
    "    lr_model = LogisticRegression()\n",
    "    lr_model.fit(X_train_tfidf, y_train)\n",
    "    joblib.dump(lr_model, lr_model_path)\n",
    "models[\"Logistic Regression\"] = lr_model\n",
    "\n",
    "# LSTM\n",
    "lstm_model_path = \"lstm_model.h5\"\n",
    "if os.path.exists(lstm_model_path):\n",
    "    print(\"Loading LSTM model...\")\n",
    "    lstm_model = load_model(lstm_model_path)\n",
    "    lstm_model.compile(loss=\"binary_crossentropy\", optimizer=Adamax(learning_rate=0.0001), metrics=[\"accuracy\"])\n",
    "    lstm_model.fit(X_train_padded, y_train, epochs=3, batch_size=32, validation_data=(X_test_padded, y_test), verbose=1)\n",
    "else:\n",
    "    print(\"Training LSTM model...\")\n",
    "    lstm_model = Sequential([\n",
    "        Embedding(vocab_size, 32, input_length=max_length),\n",
    "        Bidirectional(LSTM(32)),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    lstm_model.compile(loss=\"binary_crossentropy\", optimizer=Adamax(learning_rate=0.0001), metrics=[\"accuracy\"])\n",
    "    lstm_model.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_data=(X_test_padded, y_test), verbose=1)\n",
    "    lstm_model.save(lstm_model_path)\n",
    "models[\"LSTM\"] = lstm_model\n",
    "\n",
    "# CNN+LSTM\n",
    "cnn_lstm_model_path = \"cnn_lstm_model.h5\"\n",
    "if os.path.exists(cnn_lstm_model_path):\n",
    "    print(\"Loading CNN+LSTM model...\")\n",
    "    cnn_lstm_model = load_model(cnn_lstm_model_path)\n",
    "    cnn_lstm_model.compile(loss=\"binary_crossentropy\", optimizer=Adamax(learning_rate=0.0001), metrics=[\"accuracy\"])\n",
    "    cnn_lstm_model.fit(X_train_padded, y_train, epochs=3, batch_size=32, validation_data=(X_test_padded, y_test), verbose=1)\n",
    "else:\n",
    "    print(\"Training CNN+LSTM model...\")\n",
    "    cnn_lstm_model = Sequential([\n",
    "        Embedding(vocab_size, 32, input_length=max_length),\n",
    "        Conv1D(64, kernel_size=3, activation=\"relu\"),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Bidirectional(LSTM(32)),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    cnn_lstm_model.compile(loss=\"binary_crossentropy\", optimizer=Adamax(learning_rate=0.0001), metrics=[\"accuracy\"])\n",
    "    cnn_lstm_model.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_data=(X_test_padded, y_test), verbose=1)\n",
    "    cnn_lstm_model.save(cnn_lstm_model_path)\n",
    "models[\"CNN+LSTM\"] = cnn_lstm_model\n",
    "\n",
    "# ============================\n",
    "# Evaluation and Logging\n",
    "# ============================\n",
    "\n",
    "results = []\n",
    "dataset_name = os.path.basename(data_path)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "\n",
    "    if name == \"Logistic Regression\":\n",
    "        y_pred = model.predict(X_test_tfidf)\n",
    "    else:\n",
    "        y_pred = (model.predict(X_test_padded) > 0.5).astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"{name} Results â€” Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Dataset\": dataset_name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-score\": f1\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(log_file, mode='a', header=not os.path.exists(log_file), index=False)\n",
    "\n",
    "print(f\"\\nModel comparison results saved to '{log_file}'.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
